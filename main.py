# -*- coding: utf-8 -*-
"""ML_CP_final.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1f5ylmUVd591YPjKL4YF8uiLnm2SgQW18
"""

!pip install owlready2
!pip install docx2txt

import nltk
from nltk.tokenize import word_tokenize
from nltk import pos_tag, ne_chunk
from collections import Counter
import spacy
import docx2txt

# Download necessary NLTK resources (if not already downloaded)
nltk.download('punkt')
nltk.download('averaged_perceptron_tagger')
nltk.download('maxent_ne_chunker')
nltk.download('words')

def suggest_title(text):
    # Load the spaCy model with NER component
    nlp = spacy.load("en_core_web_sm")

    # Process the input text
    doc = nlp(text)

    # Initialize a list to store identified entities
    entities = []

    # Iterate through the entities identified by spaCy
    for ent in doc.ents:
        # Only consider entities with certain labels (e.g., PERSON, ORG, GPE)
        if ent.label_ in ['PERSON', 'ORG', 'GPE']:
            entities.append(ent.text)

    # Generate a title based on the identified entities
    if len(entities) > 0:
        title = f"Analysis of {', '.join(entities)}"
        return title
    else:
        return "No significant entities found in the text."

def extract_named_entities(text):
    words = nltk.word_tokenize(text)
    pos_tags = nltk.pos_tag(words)
    named_entities = ne_chunk(pos_tags, binary=True)  # Use binary=True to get only named entities

    entities = []

    for subtree in named_entities.subtrees():
        if subtree.label() == 'NE':
            entities.append(" ".join(word for word, tag in subtree.leaves()))

    return entities

def process_text_from_docx(file_path):
    # Extract text from Word file
    text = docx2txt.process(file_path)
    return text

# Example text
user_input = input("Enter 'file' to upload a Word file or 'text' to enter text manually: ")

if user_input.lower() == "file":
    file_path = input("Enter the path to the Word file: ")
    text_example = process_text_from_docx(file_path)
else:
    text_example = input('Enter data: ')

# Get suggested title
entities = extract_named_entities(text_example)
title = suggest_title(text_example)

print("\nNamed Entities:")
for entity in entities:
    if hasattr(entity, 'label'):
        print(entity.label(), ' '.join(e[0] for e in entity.leaves()))

# Print the suggested title
print(f"\nSuggested Title: {title}")

# Rest of your code for using spaCy for NER and visualization

# NER using spaCy
from spacy import displacy
NER = spacy.load("en_core_web_sm")
text_ner = NER(text_example)

# Print entities and their labels
for word in text_ner.ents:
    print(word.text, word.label_)

# Explain labels
print("\nExplanations:")
print("ORG:", spacy.explain("ORG"))
print("GPE:", spacy.explain("GPE"))

# Render NER visualization
displacy.render(text_ner, style="ent", jupyter=True)

import pandas as pd
# Create a DataFrame to store entities and their labels
entities_data = []
# Get entities and their labels
for word in text_ner.ents:
    entities_data.append([word.text, word.label_])
# Create a DataFrame from the data
df = pd.DataFrame(entities_data, columns=['Entity', 'Label'])
# Save the DataFrame to an Excel file
df.to_excel('entities_output.xlsx', index=False)

from collections import defaultdict
from spacy.symbols import nsubj, VERB
import networkx as nx
import matplotlib.pyplot as plt

def create_knowledge_graph(text):
    # Load the spaCy model with NLP components
    nlp = spacy.load("en_core_web_sm")

    # Process the input text
    doc = nlp(text)

    # Initialize a directed graph
    G = nx.DiGraph()

    # Iterate through the sentences in the processed text
    for sent in doc.sents:
        # Initialize subject, action, and object
        subject = ''
        action = ''
        object_ = ''

        # Iterate through the words in the sentence
        for word in sent:
            # Check if the word is a subject
            if "subj" in word.dep_:
                subject = word.text

            # Check if the word is a verb
            if word.pos == VERB:
                action = word.text

            # Check if the word is an object
            if "obj" in word.dep_:
                object_ = word.text

        # Add edges to the graph
        if subject and action:
            G.add_edge(subject, action, type="action")
        if action and object_:
            G.add_edge(action, object_, type="object")

    return G

# Add this code after you get the 'text_example' variable

# Generate Knowledge Graph
knowledge_graph = create_knowledge_graph(text_example)

# Visualize the Knowledge Graph
plt.figure(figsize=(27, 21))
pos = nx.spring_layout(knowledge_graph)
nx.draw(knowledge_graph, pos, with_labels=True, node_color='skyblue', node_size=2500, font_size=10, font_weight='bold', font_color='black', edge_color='gray')
edge_labels = nx.get_edge_attributes(knowledge_graph, 'type')
nx.draw_networkx_edge_labels(knowledge_graph, pos, edge_labels=edge_labels, font_color='red')
plt.title("Knowledge Graph")
plt.show()

from textblob import TextBlob
def aspect_based_sentiment_analysis(text):
    # Split the text into sentences
    sentences = nltk.sent_tokenize(text)

    # Initialize a list to store aspect-sentiment pairs
    aspect_sentiments = []

    for sentence in sentences:
        # Perform entity recognition
        entities = extract_named_entities(sentence)

        for aspect in entities:
            # Get the sentiment of the aspect
            sentiment = TextBlob(aspect).sentiment.polarity

            # Append aspect-sentiment pair to the list
            aspect_sentiments.append((aspect, sentiment))

    return aspect_sentiments

aspect_sentiments = aspect_based_sentiment_analysis(text_example)

# Print aspect-sentiment pairs
print("\nAspect-Based Sentiment Analysis:")
for aspect, sentiment in aspect_sentiments:
    print(f"Aspect: {aspect}, Sentiment: {sentiment}")

# Task 1: WITHOUT SENTIART and with index
def aspect_based_sentiment_analysis_1(text):
    sentences = nltk.sent_tokenize(text)
    aspect_sentiments = []

    for i, sentence in enumerate(sentences):
        entities = extract_named_entities(sentence)

        for aspect in entities:
            aspect_sentiments.append((i, aspect))

    return aspect_sentiments


# Task 2: Without added valence, WITH SENTIART AND WITH INDEX
def aspect_based_sentiment_analysis_2(text):
    sentences = nltk.sent_tokenize(text)
    aspect_sentiments = []

    for i, sentence in enumerate(sentences):
        entities = extract_named_entities(sentence)

        for aspect in entities:
            sentiment = TextBlob(aspect).sentiment.polarity
            aspect_sentiments.append((i, aspect, sentiment))

    return aspect_sentiments


# Task 3: WITH ADDED VALENCE, WITH SENTIART, WITH INDEX
def aspect_based_sentiment_analysis_3(text):
    sentences = nltk.sent_tokenize(text)
    aspect_sentiments = []

    for i, sentence in enumerate(sentences):
        entities = extract_named_entities(sentence)

        for aspect in entities:
            sentiment = TextBlob(aspect).sentiment.polarity
            aspect_sentiments.append((i, aspect, sentiment, get_valence(aspect)))

    return aspect_sentiments


# Task 4: WITHOUT SENTIART and WITHOUT INDEX
def aspect_based_sentiment_analysis_4(text):
    aspect_sentiments = []

    entities = extract_named_entities(text)

    for aspect in entities:
        aspect_sentiments.append(aspect)

    return aspect_sentiments


# Task 5: WITH SENTIART WITHOUT ADDED VALENCE AND WITHOUT INDEX
def aspect_based_sentiment_analysis_5(text):
    aspect_sentiments = []

    entities = extract_named_entities(text)

    for aspect in entities:
        sentiment = TextBlob(aspect).sentiment.polarity
        aspect_sentiments.append((aspect, sentiment))

    return aspect_sentiments

# Task 1: WITHOUT SENTIART and with index
output_1 = aspect_based_sentiment_analysis_1(text_example)
print("\nTask 1 Output:")
print(output_1)

# Task 2: Without added valence, WITH SENTIART AND WITH INDEX
output_2 = aspect_based_sentiment_analysis_2(text_example)
print("\nTask 2 Output:")
print(output_2)

# Task 3: WITH ADDED VALENCE, WITH SENTIART, WITH INDEX
output_3 = aspect_based_sentiment_analysis_3(text_example)
print("\nTask 3 Output:")
print(output_3)

# Task 4: WITHOUT SENTIART and WITHOUT INDEX
output_4 = aspect_based_sentiment_analysis_4(text_example)
print("\nTask 4 Output:")
print(output_4)

# Task 5: WITH SENTIART WITHOUT ADDED VALENCE AND WITHOUT INDEX
output_5 = aspect_based_sentiment_analysis_5(text_example)
print("\nTask 5 Output:")
print(output_5)

aspects = [item[1] for item in output_1]
aspect_counts = Counter(aspects)

# Separate aspects and their counts
aspects = list(aspect_counts.keys())
counts = list(aspect_counts.values())

# Create a bar chart
plt.figure(figsize=(32, 25))
plt.bar(aspects, counts)
plt.xlabel('Aspect')
plt.ylabel('Count')
plt.title('Count vs Aspect')
plt.xticks(rotation=45)
plt.show()